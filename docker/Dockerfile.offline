# TinyCode Offline-Ready Docker Image
# Pre-loads all models for air-gapped deployment

FROM python:3.11-slim as model-downloader

# Install build dependencies
RUN apt-get update && apt-get install -y \
    curl \
    wget \
    git \
    && rm -rf /var/lib/apt/lists/*

# Install Ollama
RUN curl -fsSL https://ollama.ai/install.sh | sh

# Create app directory
WORKDIR /app

# Copy requirements first for better caching
COPY requirements.txt .

# Install Python dependencies
RUN pip install --no-cache-dir -r requirements.txt

# Download embedding models
RUN python -c "
from sentence_transformers import SentenceTransformer
import os
print('Pre-downloading embedding models...')
model = SentenceTransformer('all-MiniLM-L6-v2')
print(f'Model cached at: {model.cache_folder}')
print('âœ… Embedding models ready')
"

# Start Ollama service and download models
RUN ollama serve & \
    sleep 10 && \
    ollama pull tinyllama:latest && \
    ollama pull nomic-embed-text:latest && \
    pkill ollama

# Verify models are downloaded
RUN ls -la ~/.ollama/models/ && \
    ls -la ~/.cache/huggingface/hub/

# Production stage
FROM python:3.11-slim as production

# Install runtime dependencies
RUN apt-get update && apt-get install -y \
    curl \
    && rm -rf /var/lib/apt/lists/*

# Create non-root user
RUN groupadd -r tinyllama && useradd -r -g tinyllama -s /bin/false tinyllama

# Install Ollama (runtime only)
RUN curl -fsSL https://ollama.ai/install.sh | sh

# Create app directory
WORKDIR /app

# Copy Python dependencies from model-downloader stage
COPY --from=model-downloader /usr/local/lib/python3.11/site-packages/ /usr/local/lib/python3.11/site-packages/

# Copy downloaded models
COPY --from=model-downloader /root/.ollama/ /home/tinyllama/.ollama/
COPY --from=model-downloader /root/.cache/ /home/tinyllama/.cache/

# Copy application code
COPY . .

# Copy offline verification script
COPY scripts/verify_offline_models.py /app/scripts/

# Set permissions
RUN chown -R tinyllama:tinyllama /app /home/tinyllama

# Create data directories
RUN mkdir -p /app/data/{plans,backups,audit_logs,index} && \
    chown -R tinyllama:tinyllama /app/data

# Expose ports
EXPOSE 8000 11434

# Health check
HEALTHCHECK --interval=30s --timeout=10s --start-period=60s --retries=3 \
    CMD curl -f http://localhost:8000/health || exit 1

# Switch to non-root user
USER tinyllama

# Set environment variables
ENV PYTHONPATH=/app
ENV PYTHONUNBUFFERED=1
ENV OLLAMA_HOST=localhost
ENV OLLAMA_PORT=11434
ENV HOME=/home/tinyllama

# Start script
COPY docker/start-offline.sh /app/start-offline.sh
RUN chmod +x /app/start-offline.sh

CMD ["/app/start-offline.sh"]